### 神经网络的学习

#### 4.1 learn from data

#### 4.2 Loss 函数

##### 4.2.1均方误差


$$
E=\frac{1}{2} \sum_k\left(y_k-t_k\right)^2
$$

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

##### 4.2.2交叉熵误差


$$
E=-\sum_k t_k \log y_k
$$

```python
def cross_entropy_error(y, t):
	delta = 1e-7
	return -np.sum(t * np.log(y + delta))
```

这里，参数y和t是NumPy数组。函数内部在计算np.log时，加上了一个微小值delta。这是因为，当出现np.log(0)时， np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。作为保护性对策，添加一个微小值可以防止负无限大的发生。

如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式：


$$
E=-\frac{1}{N} \sum_n \sum_k t_{n k} \log y_{n k}
$$

这里, 假设数据有N 个，tnk 表示第n 个数据的第k 个元素的值（ynk 是神经网络的输出，tnk 是监督数据）。式子虽然看起来有一些复杂，其实只是把求单个数据的损失函数的式（4.2）扩大到了N 份数据，不过最后还要除以N进行正规化。通过除以N，可以求单个数据的“平均损失函数”。通过这样的平均化，可以获得和训练数据的数量无关的统一指标。

##### 4.2.5 Why loss?

在参数更新时，使用的是梯度下降，即微分，精度是离散量，无法微分优化。

#### 4.3梯度下降

数值微分，数值梯度，梯度为指向最小值的方向，梯度更新步长为学习率

现在, 我们尝试用数学式来表示梯度法, 如式 (4.7) 所示。

$$
\begin{aligned}
& x_0=x_0-\eta \frac{\partial f}{\partial x_0} \\
& x_1=x_1-\eta \frac{\partial f}{\partial x_1}
\end{aligned}
$$

式 (4.7) 的 $\eta$ 表示更新量, 在神经网络的学习中, 称为学习率 (learning rate）。学习率决定在一次学习中, 应该学习多少, 以及在多大程度上更新参数。

像学习率这样的参数称为超参数。这是一种和神经网络的参数 (权重和偏置) 性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的, 学习率这样的超参数则是人工设定的。一般来说, 超参数需要尝试多个值, 以便找到一种可以使学习顺利进行的设定。

#### 4.5神经网络的学习实现

##### 4.5.1 Pipeline

前提
神经网络存在合适的权重和偏置, 调整权重和偏置以便拟合训练数据的过程称为 “学习”。神经网络的学习分成下面 4 个步骤。

步骤 1 ( mini-batch)
从训练数据中随机选出一部分数据, 这部分数据称为 mini-batch。我们的目标是减小 mini-batch 的损失函数的值。

步骤 2 (计算梯度)
为了减小 mini-batch 的损失函数的值, 需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。

步骤3 (更新参数 )
将权重参数沿梯度方向进行微小更新。

步骤 4 (重复)
重复步骤1、步骤2、步骤 3 。
